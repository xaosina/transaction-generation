#!/usr/bin/python

import pandas as pd
import numpy as np
from scipy import stats
from pathlib import Path
from argparse import ArgumentParser
from typing import Dict, List, Optional
import argparse

CONF_LEVEL = 0.01

def parse_args():
    parser = ArgumentParser()
    parser.add_argument(
        "-o", "--orig",
        help="Path to original (test) dataset csv file",
        type=Path, required=True
    )
    parser.add_argument(
        "-d", "--data", 
        help="generated data csv path", 
        type=Path, required=True
    )
    parser.add_argument("--recover-len", type=int, default=None)
    parser.add_argument("--subsample-len", default=None)
    parser.add_argument("--max-n-seqs", default=None)
    parser.add_argument("--seed", default=None)
    args = parser.parse_args()
    
    if args.subsample_len:
        args.subsample_len = list(map(int, args.subsample_len.split(",")))
    else:
        args.subsample_len = [0]
        
    if args.max_n_seqs:
        args.max_n_seqs = list(map(int, args.max_n_seqs.split(",")))
    else:
        args.max_n_seqs = [0]
        
    if args.seed:
        args.seeds = list(map(int, args.seed.split(",")))
    else:
        args.seeds = [0]
    
    return args


def test_for_exp(rvs):
    # https://stats.stackexchange.com/questions/32061/what-are-the-standard-statistical-tests-to-see-if-data-follows-exponential-or-no
    rvs = rvs[rvs > 0]
    n = len(rvs)
    if n <= 1:
        return np.nan
    bn = 2 * n / (1 + (n + 1) / (6 * n))
    stat = bn * (np.log(np.mean(rvs)) - np.mean(np.log(rvs)))
    cdf_val = stats.distributions.chi2.cdf(stat, n - 1)
    pval = 2 * np.minimum(cdf_val, 1 - cdf_val)
    return pval


def holm_bonferroni(pvals):
    sorted_pvals = pvals.sort_values()
    notna = np.sum(~np.isnan(sorted_pvals))
    correction = notna - np.arange(len(sorted_pvals))
    correction = pd.Series(data=correction, index=sorted_pvals.index)
    return np.minimum(pvals * correction.loc[pvals.index], 1)


def get_gt_randomness(path_to_test):
    return (
        pd.read_csv(path_to_test)
        .set_index(["user_id", "mcc_code", "currency_rk"], append=True)
        .groupby(["user_id", "mcc_code", "currency_rk"])
        .days_since_first_tx.diff()
        .dropna()
        .groupby(["mcc_code"])
        .agg(test_for_exp)
        .fillna(1.0)
        .pipe(holm_bonferroni)
        .pipe(lambda s: s < CONF_LEVEL)
        .rename("gt")
    )


def calc_tct(
    mcc_randomness_gt,
    df,  # dataframe with data to assess
    # for synthetic data, eg set to 32 if generated by tabsyn_32; if real, set to None
    recover_len: int | None = None,
    subsample_len: int = 0,  # 0 or negative for no subsampling
    max_n_seqs: int = 0,  # cap the maximum number of sequences in dataset, 0 or negative for no capping
    seed: int = 0,
):
    gen = np.random.default_rng(seed)
    
    print(
        "\nStarting with params: "
        f"recover_len={recover_len}, "
        f"subsample_len={subsample_len}, "
        f"max_n_seqs={max_n_seqs}, "
        f"seed={seed}."
    )
    if recover_len:  # synthetic data
        print(f"\t├ setting equal user_id to groups of {recover_len} successive rows", end="...", flush=True)
        df = (
            df
            .assign(user_id=lambda df: np.arange(len(df)) // recover_len)
            .assign(days_since_first_tx=lambda df: df.groupby("user_id").time_diff_days.cumsum())
        )
        print("done")

    if subsample_len > 0:
        print(f"\t├ subsampling {subsample_len} successive transactions from each sequence", end="...", flush=True)
        def subsample(df):
            if len(df) < subsample_len:
                return df.iloc[:0]
            max_start_idx = len(df) - subsample_len
            start = gen.integers(0, max_start_idx + 1)
            return df.iloc[start:start + subsample_len]

        df = df.groupby("user_id", group_keys=False).apply(subsample)
        print("done")

    users = df.user_id.unique()
    print(f"\t├ data contains {len(users)} sequences and {len(df)} transactions")
    if 0 < max_n_seqs < len(df):
            print(f"\tsampling random {max_n_seqs} sequences", end="...", flush=True)
            user_sample = pd.Series(gen.choice(users, size=max_n_seqs, replace=False), name="user_id")
            df = df.merge(user_sample, on="user_id")
            print("done")
            print(f"\t├ now data contains {df.user_id.nunique()} sequences and {len(df)} transactions")

    print("\t├ calculating randomness", end="...", flush=True)
    mcc_randomness = (
        df
        .set_index(["user_id", "mcc_code", "currency_rk"])
        .groupby(["user_id", "mcc_code", "currency_rk"])
        .days_since_first_tx.diff()
        .dropna()
        .groupby(["mcc_code"])
        .agg(test_for_exp)
        .fillna(1.0)
        .pipe(holm_bonferroni)
        .pipe(lambda s: s < CONF_LEVEL)
    )
    print(f"done. Mean randomness is {mcc_randomness.mean():.2%}")

    randomness_accuracy = (
        pd.concat(
            (mcc_randomness_gt, mcc_randomness),
            keys=["gt", "pred"],
            axis=1,
        ).pipe(lambda df: df.eval("(gt == pred).sum()") / len(df))
    )
    print("DONE!")
    print(f"Randomness accuracy is {randomness_accuracy:.2%}")

    return randomness_accuracy

def run_eval_tct(
    data: Path,
    orig: Path,
    recover_len: int,
    subsample_len: List[int] | None = None,
    max_n_seqs: List[int] | None = None,
    seeds: List | None = None,
) -> Dict:
    """Computes tct metric.

    Args:
        data (Path): Path to generated data csv.
        orig (Path): Path to original (test) data csv.
        recover_len (int): lengths of generated sequences (16, 32, 64 etc.).
        subsample_len (list[int]): lengths to subsample. 
            Defaults to [0] which means subsample_len == recover_len.
        seeds (list): seeds to average metrics.


    Returns:
        dict with metrics' dataframes for each smax_n_seqs

    """
    args = argparse.Namespace
    args.path_to_test = str(orig)
    args.path_to_csv = str(data)
    args.recover_len = recover_len
    args.subsample_len = subsample_len if subsample_len else [0]
    args.max_n_seqs = max_n_seqs if max_n_seqs else [0]
    args.seeds = seeds if seeds else [0]

    print(f"Getting test randomness from {args.path_to_test}", end="...", flush=True)
    mcc_randomness_gt = get_gt_randomness(args.path_to_test)
    print("done")
    
    print(f"Reading {args.path_to_csv}", end="...", flush=True)
    df = pd.read_csv(args.path_to_csv)
    print("done")

    seqs2res = dict()
    
    for mns in args.max_n_seqs:
        res_df = pd.DataFrame(
            None, 
            index=args.subsample_len, 
            columns=args.seeds, dtype=float)
        for sl in args.subsample_len:
            for seed in args.seeds:
                res_df.loc[sl, seed] = calc_tct(
                    mcc_randomness_gt,
                    df,
                    recover_len=args.recover_len,
                    subsample_len=sl,
                    max_n_seqs=mns,
                    seed=seed,
                )
        res_df_mean = res_df.mean(axis=1)
        res_df_var = res_df.var(axis=1)
        res_df['mean'] = res_df_mean
        res_df['var'] = res_df_var
        seqs2res[mns] = res_df
    
    return seqs2res


if __name__ == "__main__":
    
    args = parse_args()
    
    seqs2res = run_eval_tct(
        data=args.data,
        orig=args.orig,
        recover_len=args.recover_len,
        subsample_len=args.subsample_len,
        max_n_seqs=args.max_n_seqs,
        seeds=args.seeds)
    
    if len(seqs2res) == 1:
        print(list(seqs2res.values())[0])
    else:
        for k, v in seqs2res.items():
            print(f'Results for "max_n_seq={k}":')
            print(v)
    

        
