import torch
import torch.nn as nn
import math
from .temporal_pos_enc import ContinuousSinusoidalPosEmb


class TimeDenoisingModule(nn.Module):
    def __init__(self, transformer_dim=32, num_classes=10, n_steps=100, transformer_heads=2,
                 dim_feedforward=64, dropout=0.1, n_decoder_layers=1, device='cuda', batch_first=True,
                 x_dim=2, e_dim=2):  # ğŸš€ æ–°å¢ x_dim å’Œ e_dim å‚æ•°
        """
        :param transformer_dim: 
        :param num_classes:
        :param n_steps:
        # ... (å…¶ä»–å‚æ•°ä¿æŒä¸å˜)
        :param x_dim: x çš„ç‰¹å¾ç»´åº¦ (å¦‚æœ x æ˜¯åŸå§‹æ—¶é—´é—´éš”ï¼Œx_dim=1)
        :param e_dim: e çš„ç‰¹å¾ç»´åº¦ (å¦‚æœ e æ˜¯äº‹ä»¶ç±»å‹ IDï¼Œe_dim=1)
        """
        super(TimeDenoisingModule, self).__init__()

        self.device = device
        self.transformer_dim = transformer_dim
        self.x_dim = x_dim
        self.e_dim = e_dim

        self.num_classes_dict = num_classes
        self.cat_emb = nn.ModuleDict() 
        for key,value in self.num_classes_dict.items():
            self.cat_emb[key] = nn.Embedding(value+1, int(transformer_dim/8))

        # ğŸš€ ç§»é™¤åŸå§‹çš„ event_emb å’Œ temporal_enc ç›¸å…³çš„é€»è¾‘ï¼Œå› ä¸º x å’Œ e å·²æ˜¯å¤šç‰¹å¾è¾“å…¥
        # é™¤éä½ çš„ e ä»ç„¶æ˜¯ event IDï¼Œé‚£ä¹ˆ event_emb éœ€è¦ä¿®æ”¹è¾“å‡ºç»´åº¦
        
        # å¦‚æœ e ä»æ˜¯äº‹ä»¶ IDï¼Œåˆ™ä½¿ç”¨ Embedding Layer
        # if e_dim == 1:
        #     self.event_input_layer = nn.Embedding(num_classes + 1, int(transformer_dim/4), padding_idx=num_classes)
        # else:
        #     # ğŸš€ å¦‚æœ e æ˜¯å¤šç‰¹å¾åºåˆ—ï¼Œä½¿ç”¨çº¿æ€§å±‚æ˜ å°„åˆ°ç»Ÿä¸€ç»´åº¦
        #     self.event_input_layer = nn.Linear(e_dim, int(transformer_dim/4))
            
        # ğŸš€ çº¿æ€§å±‚ï¼šå°† x çš„å¤šç‰¹å¾ç»´åº¦æ˜ å°„åˆ°ç»Ÿä¸€ç»´åº¦
        self.x_input_layer = nn.Linear(x_dim, int(transformer_dim/4))

        # Diffusion time embedding: ä¿æŒä¸å˜
        self.time_pos_emb = ContinuousSinusoidalPosEmb(int(transformer_dim/4), n_steps)
        self.mlp = nn.Sequential(
            nn.Linear(int(transformer_dim/4), transformer_dim),
            nn.Softplus(),
            nn.Linear(transformer_dim, int(transformer_dim/4))
        )

        self.nhead = transformer_heads

        # Decoder: ä¿æŒä¸å˜
        decoder_layer = nn.TransformerDecoderLayer(d_model=transformer_dim, nhead=transformer_heads,
                                                   dim_feedforward=dim_feedforward, dropout=dropout)

        decoder_norm = nn.LayerNorm(transformer_dim)
        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=n_decoder_layers,
                                             norm=decoder_norm)
        
        # ğŸš€ ç»Ÿä¸€ç‰¹å¾ç»´åº¦å±‚ï¼šå°†æ‰€æœ‰ç¼–ç åçš„ç‰¹å¾ (x_emb, e_emb, t_emb, order_emb) æ‹¼æ¥åçš„ç»´åº¦æ˜ å°„åˆ° transformer_dim
        # è¿™é‡Œçš„è¾“å…¥ç»´åº¦æ˜¯ 4 * (transformer_dim/4) = transformer_dim
        # æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªçº¿æ€§å±‚æ¥å¤„ç†æ‹¼æ¥åçš„ç‰¹å¾ã€‚
        self.unified_feature_layer = nn.Linear(4 * int(transformer_dim/4), transformer_dim)

        self.output_layer = nn.Linear(transformer_dim, x_dim)

        self.num_classes = num_classes
        
        # Time encoding for inter-arrival time seq
        # ğŸš€ ç§»é™¤åŸå§‹çš„ position_vec å’Œ order_vec çš„å®šä¹‰ï¼Œå› ä¸ºæˆ‘ä»¬ç”¨æ›´æ ‡å‡†çš„ Order/Position Encoding

    def forward(self, x, e, t, hist,cat_list):
        """
        :param x: B x Seq_Len x X_Dim (å¦‚æœ X_Dim > 1)
        :param e: B x Seq_Len x E_Dim (å¦‚æœ E_Dim > 1)
        :param t: B
        :param hist: history representation, B x L_context x Dim
        """
        # 1. æ—¶é—´æ­¥ t ç¼–ç  (ä¿æŒä¸å˜ï¼Œè¾“å‡º B x d_model/4)
        t_emb = self.time_pos_emb(t)
        t_emb = self.mlp(t_emb) # B x d_model/4

        # 2. é¡ºåºç¼–ç  order (è¾“å‡º B x Seq_Len x d_model/4)
        order = torch.arange(x.size(1), device=x.device).unsqueeze(0).repeat(x.size(0), 1)
        order_emb = self.order_enc(order.float())

        # 3. å°† t æ‰©å±•åˆ°åºåˆ—é•¿åº¦ (è¾“å‡º B x Seq_Len x d_model/4)
        t_emb_seq = t_emb.view(x.size(0), 1, -1).repeat(1, x.size(1), 1)

        # 4. x ç‰¹å¾ç¼–ç  (è¾“å‡º B x Seq_Len x d_model/4)
        x_emb = self.x_input_layer(x.float()) # ğŸš€ ä½¿ç”¨çº¿æ€§å±‚æ˜ å°„

        # 5. e ç‰¹å¾ç¼–ç  (è¾“å‡º B x Seq_Len x d_model/4)
        # if self.e_dim == 1:
        #     e_emb = self.event_input_layer(e.long()) # ğŸš€ åŸå§‹åµŒå…¥å±‚ (å‡è®¾ e æ˜¯ ID)
        # else:
        #     e_emb = self.event_input_layer(e.float()) # ğŸš€ çº¿æ€§å±‚ (å‡è®¾ e æ˜¯å¤šç‰¹å¾)
    
        e_emb = []

        idx = 0 
        for key,value in self.num_classes_dict.items():
            e_temp = e[:,:,idx]
            e_emb.append(self.cat_emb[key](e_temp))
            idx += 1
        
        e_emb = torch.cat(e_emb,dim=-1)

        # 6. æ‹¼æ¥æ‰€æœ‰ç‰¹å¾ (B x Seq_Len x (4 * d_model/4))
        combined_features = torch.cat([x_emb, e_emb, t_emb_seq, order_emb], dim=-1)
        
        # 7. ç»Ÿä¸€ç»´åº¦åˆ° transformer_dim (B x Seq_Len x transformer_dim)
        tgt = self.unified_feature_layer(combined_features) # ğŸš€ æ–°å¢ç»Ÿä¸€å±‚

        # Transformer Decoder éƒ¨åˆ† (ä¿æŒä¸å˜ï¼Œé™¤äº† memory ç»´åº¦æ£€æŸ¥)
        
        tgt_mask = self.generate_square_subsequent_mask(x.size(1)).to(x.device)
        
        memory = hist # å‡è®¾ hist å·²ç»æ˜¯ B x L_context x transformer_dim

        # memory çš„ç»´åº¦æ£€æŸ¥éœ€è¦ä¿®æ”¹ï¼Œå› ä¸ºå®ƒç°åœ¨å¯èƒ½ä¸æ˜¯ transformer_dim
        # å‡è®¾ hist å·²ç»æ˜¯ B x L_context x transformer_dim
        assert memory.size(2) == self.transformer_dim, \
            f'Error: history dim (got {memory.size(2)}) should equal to transformer_dim (got {self.transformer_dim})'

        # ç»´åº¦è½¬æ¢ï¼šB x S x D -> S x B x D
        memory = hist.permute(1, 0, -1)
        tgt = tgt.permute(1, 0, -1)

        output = self.decoder(tgt, memory, tgt_mask=tgt_mask)

        # ç»´åº¦è½¬æ¢ï¼šS x B x D -> B x S x D
        output = output.permute(1, 0, -1)

        out = self.output_layer(output)

        return out

    def generate_square_subsequent_mask(self, sz):
        r"""Generate a square mask for the sequence. The masked positions are filled with float('-inf').
            Unmasked positions are filled with float(0.0).
        """
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def diff_step_enc(self, time, shape):
        """
        Input: batch*seq_len.
        Output: batch*seq_len*d_model.
        """
        result = time.unsqueeze(-1) * torch.ones(shape).to(self.device)
        result = result[:, 1, :].to(self.device)
        result[:, 0::2] = torch.sin(result[:, 0::2])
        result[:, 1::2] = torch.cos(result[:, 1::2])
        return result.unsqueeze(1)

    def temporal_enc(self, dt_seq):
        """
        dt_seq: batch*seq_len.
        Output: batch*seq_len*d_model.
        """
        result = dt_seq.unsqueeze(-1) / self.position_vec
        result[:, :, 0::2] = torch.sin(result[:, :, 0::2])
        result[:, :, 1::2] = torch.cos(result[:, :, 1::2])
        return result
    

    
    def order_enc(self, order_seq):
        """
        ğŸš€ ä¿æŒåŸå§‹çš„ Order Encoding é€»è¾‘ï¼Œä½†ä½¿ç”¨ order_vec ä¿æŒä¸å˜ï¼Œå®ƒæ˜¯ä¸€ä¸ª d_model ç»´åº¦çš„ç¼–ç 
        """
        d_model = self.transformer_dim
        # ä½¿ç”¨ transformer_dim ä½œä¸º d_model
        position_vec = torch.tensor(
            [math.pow(10000.0, 2.0 * (i // 2) / d_model) for i in range(d_model)],
            device=order_seq.device) # ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        
        result = order_seq.unsqueeze(-1) / position_vec[:int(d_model/4)] # åªå– d_model/4 ç»´
        # åŸå§‹ä»£ç çš„ order_vec ç»´åº¦æ˜¯ transformer_dimï¼Œä½†ç”¨äº d_model/4 ç¼–ç ï¼Œè¿™é‡Œä¿æŒ d_model/4
        
        # æˆ‘ä»¬ä¸º order_enc ä¿æŒ d_model/4 çš„è¾“å‡ºç»´åº¦ï¼Œç„¶åå’Œå…¶ä»–ç‰¹å¾æ‹¼æ¥ã€‚
        
        result = order_seq.unsqueeze(-1) / position_vec[:int(d_model/4)] 
        
        order_emb = torch.zeros(order_seq.size(0), order_seq.size(1), int(d_model/4), device=order_seq.device)
        order_emb[:, :, 0::2] = torch.sin(result[:, :, 0::2])
        order_emb[:, :, 1::2] = torch.cos(result[:, :, 1::2])
        return order_emb
