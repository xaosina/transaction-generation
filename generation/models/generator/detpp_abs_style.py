from copy import deepcopy
from dataclasses import replace

import torch
import torch.nn as nn
from ebes.model import BaseSeq2Seq
from ebes.types import Seq

from generation.models import autoencoders
from generation.utils import freeze_module

from ...data.data_types import GenBatch, LatentDataConfig, PredBatch, valid_mask
from ..encoders import AutoregressiveEncoder
from . import BaseGenerator, ModelConfig


class ConditionalHeadS(BaseSeq2Seq):
    """
    ä¿®æ”¹åŽçš„ Headï¼šæ”¯æŒæŽ¥æ”¶æ‹¼æŽ¥äº† Style å‘é‡çš„ä¸Šä¸‹æ–‡ã€‚
    """

    def __init__(self, context_size, style_size, k):
        super().__init__()
        self.context_size = context_size # D_gru
        self.style_size = style_size     # D_vae (z_style)
        
        # æ‹¼æŽ¥åŽçš„æ€»è¾“å…¥ç»´åº¦
        self.total_input_dim = context_size + style_size 
        
        # ä¿®æ”¹ projection å±‚ä»¥æŽ¥å—æ€»ç»´åº¦
        # è¾“å…¥: (Context + Style) + Queries(æˆ‘ä»¬å‡è®¾Queriesç»´åº¦ä¸ŽContextä¸€è‡´æˆ–ç‹¬ç«‹å®šä¹‰)
        # è¿™é‡Œæˆ‘ä»¬è®¾å®š Queries ç»´åº¦ç­‰äºŽ context_size
        self.proj = torch.nn.Linear(self.total_input_dim + context_size, context_size)
        self.relu = torch.nn.ReLU()

        # Queries ä¿æŒä¸Ž GRU è¾“å‡ºç»´åº¦ä¸€è‡´ï¼Œæˆ–è€…æ˜¯ç‹¬ç«‹çš„ç»´åº¦
        self.queries = torch.nn.Parameter(torch.randn(k, context_size)) 
        self.k = k

    @property
    def output_dim(self):
        return self.context_size * self.k

    def forward_impl(self, ctx):
        # ctx shape: [B, D_gru + D_style]
        b, d = ctx.shape
        assert d == self.total_input_dim, f"Input dim mismatch: expected {self.total_input_dim}, got {d}"

        # 1. å‡†å¤‡ Queries: [B, K, D_gru]
        x = self.queries[None].repeat(b, 1, 1) 
        
        # 2. å‡†å¤‡ Context: [B, 1, D_gru + D_style] -> [B, K, D_gru + D_style]
        ctx_expanded = ctx.unsqueeze(1).repeat(1, self.k, 1)
        
        # 3. æ‹¼æŽ¥ Context å’Œ Queries
        # Result: [B, K, (D_gru + D_style) + D_gru]
        combined = torch.cat([ctx_expanded, x], -1)
        
        # 4. Flatten and Project
        combined = combined.flatten(0, 1) # (BK, D_total + D_query)
        out = self.proj(combined)         # (BK, D_out)
        out = self.relu(out)
        
        return out.reshape(b, self.output_dim) # (B, KO)

    def forward(self, seq: Seq):
        mask = valid_mask(seq)
        x = seq.tokens
        assert x.ndim > 2  # (L, B, D_total).
        shape = list(x.shape)
        x_masked = x[mask]  # (V, D_total).
        v = len(x_masked)
        
        # å¤„ç† Masked åºåˆ—
        x_mapped = self.forward_impl(x_masked.flatten(0, -2)).reshape(
            *([v] + shape[2:-1] + [self.output_dim])
        ) 
        
        x_new = torch.zeros(
            *[shape[:-1] + [self.output_dim]],
            dtype=x_mapped.dtype,
            device=x_mapped.device
        )
        x_new[mask] = x_mapped
        return replace(seq, tokens=x_new)


class DeTPP_abs_style(BaseGenerator):
    def __init__(self, data_conf: LatentDataConfig, model_config: ModelConfig):
        super().__init__()

        # --- 1. åˆå§‹åŒ– VAE (Frozen) ---
        self.autoencoder = getattr(autoencoders, model_config.autoencoder.name)(
            data_conf, model_config
        )
        self.autoencoder_name = model_config.autoencoder.name
        if model_config.autoencoder.checkpoint:
            ckpt = torch.load(model_config.autoencoder.checkpoint, map_location="cpu")
            self.autoencoder.load_state_dict(ckpt["model"], strict=False)
        
        # å¼ºåˆ¶å†»ç»“ VAEï¼Œä¸ç®¡é…ç½®æ–‡ä»¶æ€Žä¹ˆå†™ï¼Œä¸ºäº†ä¿è¯é€»è¾‘æ­£ç¡®
        self.autoencoder = freeze_module(self.autoencoder)

        # èŽ·å–ç»´åº¦
        self.vae_dim = self.autoencoder.encoder.output_dim # D_vae (ä¹Ÿæ˜¯ Style ç»´åº¦)
        
        # --- 2. åˆå§‹åŒ– GRU (Encoder) ---
        encoder_params = model_config.latent_encoder.params or {}
        encoder_params["input_size"] = self.vae_dim 

        self.encoder = AutoregressiveEncoder(
            model_config.latent_encoder.name, encoder_params
        )
        # èŽ·å– GRU çš„è¾“å‡ºç»´åº¦
        self.gru_dim = self.encoder.output_dim

        # --- 3. åˆå§‹åŒ– Head ---
        k_factor = model_config.params["k_factor"]
        assert k_factor >= 1
        self.k_output = int(k_factor * data_conf.generation_len)
        self.k_gen = model_config.params.get("k_gen") or data_conf.generation_len
        
        # å…³é”®ä¿®æ”¹ï¼šä¼ å…¥ context_size å’Œ style_size
        self.next_k_head = ConditionalHeadS(
            context_size=self.gru_dim, 
            style_size=self.vae_dim, 
            k=self.k_output
        )
        
        # Presence Head ä¹Ÿéœ€è¦èƒ½å¤Ÿå¤„ç†æ‹¼æŽ¥åŽçš„ç»´åº¦ (æˆ–è€…åªå¤„ç† GRU ç»´åº¦ï¼Œçœ‹å…·ä½“è®¾è®¡)
        # ä¸ºäº†ç®€å•å’Œä¸€è‡´ï¼Œå»ºè®® Presence Head ä¹ŸæŽ¥æ”¶æ‹¼æŽ¥ç»´åº¦ï¼Œæˆ–è€…åªç”¨ projection
        # è¿™é‡Œå‡è®¾ Presence åªéœ€è¦ GRU ä¸Šä¸‹æ–‡å³å¯åˆ¤æ–­æ˜¯å¦å‘ç”Ÿï¼Œè‹¥éœ€è¦ Style ä¹Ÿå¯æ‹¼æŽ¥
        # ä¸‹é¢ä»£ç ä¿®æ”¹ä¸ºæŽ¥æ”¶æ‹¼æŽ¥åŽçš„ç»´åº¦
        self.presence_head = nn.Linear(self.gru_dim + self.vae_dim, self.k_output)

        self.gru_dim = self.encoder.output_dim
        self.vae_dim = self.autoencoder.encoder.output_dim

        # ðŸŸ¢ 1. å®šä¹‰ GRU è¾“å‡ºçš„ LayerNorm
        # nn.LayerNorm(normalized_shape) ä½œç”¨äºŽ GRU çš„ç‰¹å¾ç»´åº¦ (D_gru)
        self.norm_gru = nn.LayerNorm(self.gru_dim)

        # ðŸŸ¢ 2. å®šä¹‰ VAE æ½œåœ¨å‘é‡çš„ LayerNorm
        # ä½œç”¨äºŽ VAE çš„æ½œåœ¨ç»´åº¦ (D_vae)
        self.norm_vae = nn.LayerNorm(self.vae_dim)

    def _apply_delta(self, x: GenBatch):
        x = deepcopy(x)
        deltas = x.time
        deltas[1:,:] = deltas[1:,:] - deltas[:-1,:]
        deltas[0, :] = 0
        x.time = deltas
        return x

    def _sort_time_and_revert_delta(self, hist, pred):
        order = pred.time.argsort(dim=0)
        for attr in ["time", "num_features", "cat_features"]:
            tensor = getattr(pred, attr)
            if tensor is None: continue
            shaped_order = order.reshape(*(list(order.shape) + [1] * (tensor.ndim - order.ndim)))
            tensor = tensor.take_along_dim(shaped_order, dim=0)
            setattr(pred, attr, tensor)
        pred.time += hist.time[hist.lengths - 1, torch.arange(hist.shape[1])]
        return pred

    # è¾…åŠ©å‡½æ•°ï¼šæ‹¼æŽ¥ (Conditioning)
    def _condition_sequence(self, h_gru_seq: Seq, z_style: torch.Tensor) -> Seq:
        # h_gru_seq.tokens shape: [L, B, D_gru]
        # z_style shape:           [B, D_vae]
        
        L, B, _ = h_gru_seq.tokens.shape
        
        # ðŸŸ¢ 1. å½’ä¸€åŒ– GRU åºåˆ—
        # LayerNorm è‡ªåŠ¨ä½œç”¨äºŽæœ€åŽä¸€ä¸ªç»´åº¦ (D_gru)ï¼Œä½¿å¾—å…¶å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1
        h_gru_norm = self.norm_gru(h_gru_seq.tokens) 
        
        # ðŸŸ¢ 2. å½’ä¸€åŒ– VAE é£Žæ ¼å‘é‡
        # LayerNorm è‡ªåŠ¨ä½œç”¨äºŽæœ€åŽä¸€ä¸ªç»´åº¦ (D_vae)
        z_style_norm = self.norm_vae(z_style)        
        
        # 3. æ‰©å±• VAE é£Žæ ¼å‘é‡
        z_repeated = z_style_norm.unsqueeze(0).repeat(L, 1, 1)
        
        # 4. æ‹¼æŽ¥
        conditioned_tokens = torch.cat([h_gru_norm, z_repeated], dim=-1)
        
        return replace(h_gru_seq, tokens=conditioned_tokens)


    def forward(self, x: GenBatch) -> PredBatch:
        L, B = x.shape
        x = deepcopy(x)
        if self.autoencoder_name == "BaselineAE":
            x = self._apply_delta(x)
            
        # 1. VAE Encoder -> Z_sequence [L, B, D_vae]
        # ç”±äºŽ frozen=True ä¸”å†…éƒ¨ pretrained=Falseï¼Œè¿™é‡Œè¿”å›žçš„æ˜¯åŒ…å«é‡‡æ · Z çš„ Seq
        z_seq = self.autoencoder.encoder(x, copy=False) 
        # 2. GRU Encoder -> H_GRU [L, B, D_gru]
        h_gru_seq = self.encoder(z_seq) 
        
        # --- 3. æå– Z_style ç”¨äºŽè®­ç»ƒ (Reconstruction) ---
        # å–åºåˆ—ä¸­æœ€åŽä¸€ä¸ªæœ‰æ•ˆæ—¶é—´æ­¥çš„ Z ä½œä¸º Style çš„ä»£è¡¨
        # z_seq.tokens: [L, B, D_vae]
        last_indices = z_seq.lengths - 1
        z_style_train = z_seq.tokens[last_indices, torch.arange(B)] # [B, D_vae]
        
        # 4. æ‹¼æŽ¥ (Conditioning)
        # è¾“å…¥: GRUåºåˆ— + æå–çš„ Z_style
        x_conditioned = self._condition_sequence(h_gru_seq, z_style_train)
        
        # 5. Prediction
        # x_conditioned çš„ç»´åº¦æ˜¯ D_gru + D_vaeï¼Œç¬¦åˆ Head çš„è¦æ±‚
        x_pred = self.next_k_head(x_conditioned) # L, B, K * D
        
        x_pred = Seq(
            tokens=x_pred.tokens.reshape(L, B * self.k_output, -1),
            lengths=x_pred.lengths.repeat_interleave(self.k_output, 0),
            time=None,
        )
        
        # Presence Head ä¹Ÿä½¿ç”¨æ‹¼æŽ¥åŽçš„è¾“å…¥
        presence_scores = self.presence_head(x_conditioned.tokens).reshape(L, B, -1)
        
        # Decoder (Mapping back to features)
        x_recon = self.autoencoder.decoder(x_pred) 
        x_recon = x_recon.k_reshape(self.k_output) 

        return (x_recon, presence_scores,0.0)

    def generate(
        self,
        hist: GenBatch,
        gen_len: int,
        with_hist=False,
        topk=1,
        temperature=1.0,
    ) -> GenBatch:
        orig_hist = deepcopy(hist)
        hist = deepcopy(hist)
        already_generated = 0
        
        # --- 1. é‡‡æ ·å…¨å±€é£Žæ ¼ (å®žçŽ°å¤šæ ·æ€§) ---
        # åœ¨å¾ªçŽ¯å¤–é‡‡æ ·ä¸€æ¬¡ï¼Œä¿æŒæ•´ä¸ªç”Ÿæˆè¿‡ç¨‹é£Žæ ¼ä¸€è‡´
        B = hist.shape[1]
        # å…³é”®ç‚¹ï¼šä»Ž N(0, I) é‡‡æ ·ï¼Œè€Œä¸æ˜¯ä»Ž VAE Encoder èŽ·å–
        z_style_prior = torch.randn(B, self.vae_dim, device=hist.device)
        
        with torch.no_grad():
            for _ in range(0, gen_len, self.k_gen):
                L, B = hist.shape
                x = deepcopy(hist)
                if self.autoencoder_name == "BaselineAE":
                    x = self._apply_delta(hist)
                
                # 2. VAE Encoder (ä»…ç”¨äºŽæå–åŽ†å²ç‰¹å¾)
                # è¿™é‡Œå…¶å®žä¸éœ€è¦ stochasticityï¼Œä½†å› ä¸ºå®ƒ frozen+pretrained=Falseï¼Œå®ƒä¼šé‡‡æ ·ã€‚
                # è¿™æ²¡å…³ç³»ï¼Œå› ä¸ºæˆ‘ä»¬åªç”¨å®ƒæ¥è®¡ç®— GRU çš„è¾“å…¥ï¼Œè€Œä¸ç”¨å®ƒæ¥åš Styleã€‚
                z_seq_hist = self.autoencoder.encoder(x, copy=False)
                
                # 3. GRU Encoder -> H_GRU [1, B, D_gru] (AutoregressiveEncoder.generate è¿”å›žæœ€åŽä¸€æ­¥)
                h_gru_last = self.encoder.generate(z_seq_hist) 
                
                # 4. æ‹¼æŽ¥ (Conditioning)
                # ä½¿ç”¨æˆ‘ä»¬æ‰‹åŠ¨é‡‡æ ·çš„ z_style_prior
                x_conditioned = self._condition_sequence(h_gru_last, z_style_prior)
                
                # 5. Prediction
                x_out = self.next_k_head(x_conditioned) 
                
                # Filter events logic ... (ä¿æŒåŽŸæ ·)
                # æ³¨æ„ x_out.tokens shape æ˜¯ [1, B, K*D] -> reshape -> [K, B, D]
                x_tokens = x_out.tokens.reshape(B, self.k_output, -1).transpose(0, 1)
                
                # Presence Head è¾“å…¥ä¹Ÿè¦æ˜¯æ‹¼æŽ¥åŽçš„
                # x_conditioned.tokens [1, B, D_total] -> [B, D_total]
                p_in = x_conditioned.tokens.squeeze(0) 
                presence_scores = self.presence_head(p_in).transpose(0, 1) # [K, B]
                
                topk_indices = torch.topk(presence_scores, self.k_gen, dim=0)[1]
                x_tokens = torch.take_along_dim(x_tokens, topk_indices.unsqueeze(-1), dim=0)
                
                x_new_seq = Seq(
                    tokens=x_tokens,
                    lengths=torch.full((B,), self.k_gen, device=hist.device),
                    time=None,
                )
                
                # Reconstruct
                rec = self.autoencoder.decoder.generate(x_new_seq, topk=topk, temperature=temperature)
                
                already_generated += self.k_gen
                hist.append(rec)
        
        pred_batch = hist.tail(already_generated).head(gen_len)
        
        if with_hist:
            orig_hist.append(pred_batch)
            return orig_hist
        else:
            return pred_batch