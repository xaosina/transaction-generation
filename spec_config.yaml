# DATA
data_conf:
  cat_cardinalities:
    src_type32: 88
    src_type11: 47
    event_subtype: 60
    dst_type11: 58
    event_type: 55
    currency: 13
    dst_type12: 253
    src_type22: 84
    src_type12: 188
    # src_type31: 1455
    # src_type21: 8000
  num_names:
    - amount
  index_name: client_id
  time_name: days_since_first_tx
  focus_on:

  test_path: 'data/mbd-50k/test'
  train_path: 'data/mbd-50k/train'
  val_ratio: 0.15
  num_workers: 0
  train_resamples: 1
  batch_size: 128

  min_history_len: 32
  generation_len: &generation_len 32
  max_seq_len: 1045
  padding_value: 0

  train_random_end: none
  val_random_end: time
  
  train_transforms:
    "0":
      RescaleTime:
        loc: 0.0
        scale: 365.0
    "1":
      TimeToDiff:
        disable: false
    "2":
      Logarithm:
        names: ["amount"]
    "shuffle":
      ShuffleBatch:
        keep_last_n: -1

  val_transforms:
    "0":
      RescaleTime:
        loc: 0.0
        scale: 365.0
    "1":
      TimeToDiff:
        disable: False
    "2":
      Logarithm:
        names: ["amount"]
    shuffle:
      ShuffleBatch:
        keep_last_n: -1
    "3":
      CutTargetSequence:
        target_len: *generation_len

# MODEL
model:
  name: Generator
  preprocessor:
    cat_emb_dim: 16
    num_emb_dim: 64
    num_norm: True
    batch_transforms:
      '0':
        TimeToFeatures:
          process_type: "cat"
  encoder:
    name: GRU
  #   params:
  #     hidden_size: 256
  #     num_layers: 4
      
  # vae:
  #   num_layers: 2
  #   d_token: 6
  #   n_head: 2
  #   factor: 64


optimizer:
  name: Adam
  params:
    lr: 1.5e-3
    weight_decay: 1.e-9

schedulers:
  "step":
    StepLR:
      step_size: 30
  # "beta":
  #   BetaScheduler:
  #     init_beta: 0.01
  #     factor: 0.7
  #     patience: 4
  #     min_beta: 0.00001
  #     verbose: True

loss:
  name: baseline
  params:
    mse_weight: 0.07

evaluator:
  devices: ["cuda:1", "cuda:2"]
  metrics:
    - Reconstruction
    - Levenshtein:
        target_key: event_type
    # - Accuracy:
    #     target_key: event_type
    - F1Metric:
        target_key: event_type
        average: macro
    - F1Metric:
        target_key: event_type
        average: micro

    # - Gini:
    #     target_key: event_type
    # - ShannonEntropy:
    #     target_key: event_type

    # - CardinalityCoverage:
    #     target_key: event_type
    #     overall: False
    # - CardinalityCoverage:
    #     target_key: event_type
    #     overall: True
    # - NoveltyScore:
    #     target_key: event_type
    #     overall: False
    # - NoveltyScore:
    #     target_key: event_type
    #     overall: True

    - Density:
        log_cols: ["amount"]
        with_timediff: True
        save_details: True
        verbose: False

    # - Detection:
    #     dataset_config: detection/mbd
    #     method_config: transformer
    #     condition_len: 0
    #     verbose: True

    # - Detection:
    #     dataset_config: detection/mbd
    #     method_config: gru
    #     condition_len: *generation_len
    #     verbose: True

logging:
  file_lvl: "info"
  cons_lvl: "info"

trainer:
  total_iters: 1_000_000
  total_epochs: null
  ckpt_track_metric: &main_metric 'Reconstruction overall'
  iters_per_epoch: null
  patience: 10
  profiling: false
  verbose: True
  ckpt_resume: null


# Runner params:
run_name: debug/-
log_dir: log/generation
device: "cuda:1"
common_seed: 0

runner:
  name: GenerationTrainer
  run_type: simple
  seed_keys:
    - "common_seed"
  params:
    n_runs: 1
    n_workers: 1
  device_list:


optuna:
  params:
    n_trials: 70
    n_startup_trials: 3
    request_list: 
      - 'optimizer.params.weight_decay': 1.5429769361370426e-13
        'optimizer.params.lr': 0.00016912601015410544
        'model.encoder.params.hidden_size': 164
        'model.encoder.params.num_layers': 3
        'model.encoder.params.dropout': 0.1831228989111418
        "model.preprocessor.batch_transforms.'0'.TimeToFeatures.process_type": cat
        'model.preprocessor.num_norm': false
        'model.preprocessor.cat_emb_dim': 41
        'model.preprocessor.num_emb_dim': 60
        'loss.params.mse_weight': 0.5
        'data_conf.train_transforms.1.TimeToDiff.disable': True
    target_metric: *main_metric
  suggestions:
    - ["optimizer.params.weight_decay", ["suggest_float", {low: 1.e-15, high: 1.e-3, log: True}]]
    - ["optimizer.params.lr", ["suggest_float", {low: 1.e-5, high: 0.1, log: True}]]

    - ["model.encoder.params.hidden_size", ["suggest_int", {low: 10, high: 1200, log: True}]]
    - ["model.encoder.params.num_layers", ["suggest_int", {low: 1, high: 7, log: False}]]
    - ["model.encoder.params.dropout", ["suggest_float", {low: 1.e-10, high: 0.3, log: True}]]

    - ["model.preprocessor.batch_transforms.0.TimeToFeatures.process_type", ["suggest_categorical", {choices: ["diff", "cat", "none"]}]]
    - 
      - ["data_conf.train_transforms.1.TimeToDiff.disable", "data_conf.val_transforms.1.TimeToDiff.disable"]
      - ["suggest_categorical", {choices: [True, False]}]
    - ["model.preprocessor.num_norm", ["suggest_categorical", {choices: [True, False]}]]
    - ["model.preprocessor.cat_emb_dim", ["suggest_int", {low: 1, high: 128, log: False}]]
    - ["model.preprocessor.num_emb_dim", ["suggest_int", {low: 1, high: 128, log: False}]]

    - ["loss.params.mse_weight", ["suggest_float", {low: 0, high: 1, log: False}]]